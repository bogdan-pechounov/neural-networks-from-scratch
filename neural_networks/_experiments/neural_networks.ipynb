{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 984,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    self.W = np.zeros((input_size, output_size))\n",
    "    self.b = np.zeros(output_size)\n",
    "\n",
    "layer = Layer(4, 3)\n",
    "layer.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4]]),\n",
       " array([[51.],\n",
       "        [61.],\n",
       "        [71.]]))"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    self.W = np.zeros((input_size, output_size))\n",
    "    self.b = np.zeros(output_size)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    return np.dot(self.W.T, input) + self.b\n",
    "\n",
    "layer = Layer(4, 3)\n",
    "layer.W = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]])\n",
    "layer.b = np.ones((3, 1))\n",
    "input = np.array([[1, 2, 3, 4]]).T\n",
    "input, layer.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]), array([52, 62, 72]))"
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(4, 3)\n",
    "layer.W = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]])\n",
    "layer.b = np.array([2, 2, 2])\n",
    "\n",
    "input = np.array([1, 2, 3, 4])\n",
    "input, layer.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2, 3, 4]]), array([[51., 61., 71.]]))"
      ]
     },
     "execution_count": 987,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# switch to row vectors\n",
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    self.W = np.zeros((input_size, output_size))\n",
    "    self.b = np.zeros(output_size)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "layer = Layer(4, 3)\n",
    "layer.W = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]])\n",
    "layer.b = np.ones((1,3))\n",
    "\n",
    "input = np.array([[1, 2, 3, 4]])\n",
    "input, layer.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]), array([52, 62, 72]))"
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(4, 3)\n",
    "layer.W = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7], [7, 8, 9]])\n",
    "layer.b = np.array([2, 2, 2])\n",
    "\n",
    "input = np.array([1, 2, 3, 4])\n",
    "input, layer.forward(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45.])"
      ]
     },
     "execution_count": 989,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "  def __init__(self):\n",
    "    self.layers = []\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def forward(self, input):\n",
    "    for layer in self.layers:\n",
    "      # output of current layer becomes input of next layer\n",
    "      input = layer.forward(input)\n",
    "    return input\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 1))\n",
    "\n",
    "# set weights to 1 instead of 0\n",
    "for layer in nn.layers:\n",
    "  layer.W += 1\n",
    "\n",
    "input = np.array([1,2,3,4,5])\n",
    "nn.forward(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 1))\n",
    "\n",
    "age = 20\n",
    "weight = 200\n",
    "some_medical_number = 0.5\n",
    "some_medical_number2 = 114\n",
    "some_medical_number3 = -7.12\n",
    "\n",
    "input = np.array([age, weight, some_medical_number, some_medical_number2, some_medical_number3])\n",
    "prediction = nn.forward(input)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.75057531, -0.50684212, -0.48841224,  0.92122166,  1.09457649],\n",
       "        [-3.50109046,  0.09410201, -0.52806599, -1.05141354, -0.70532309],\n",
       "        [-0.07247148, -1.29050858,  0.21669748,  1.33593511,  0.84481254]]),\n",
       " array([[  0.80990542],\n",
       "        [-17.07537319],\n",
       "        [  3.10339523]]))"
      ]
     },
     "execution_count": 991,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork()\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 1))\n",
    "\n",
    "for layer in nn.layers:\n",
    "  layer.W += 1\n",
    "\n",
    "sample_1 = np.random.randn(5)\n",
    "sample_2 = np.random.randn(5)\n",
    "sample_3 = np.random.randn(5)\n",
    "samples = np.array([sample_1, sample_2, sample_3])\n",
    "\n",
    "samples, nn.forward(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13999999999999999"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MSE():\n",
    "  def loss(self, Y_hat, Y):\n",
    "    return np.mean(np.square(Y - Y_hat))\n",
    "\n",
    "predictions = np.array([0.5, 2.4, 2.9])\n",
    "true_values = np.array([1, 2, 3])\n",
    "\n",
    "error = MSE()\n",
    "error.loss(predictions, true_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.105\n",
      "[[-0.25  0.2 ]\n",
      " [ 0.05  3.  ]]\n"
     ]
    }
   ],
   "source": [
    "class MSE():\n",
    "  @staticmethod\n",
    "  def loss(Y_hat, Y):\n",
    "    return np.mean(np.square(Y - Y_hat))\n",
    "\n",
    "  @staticmethod\n",
    "  def loss_derivative(Y_hat, Y):\n",
    "    n = Y_hat.size\n",
    "    return 2 * (Y_hat - Y) /n\n",
    "\n",
    "predictions = np.array([[0.5, 2.4], [3.1, 10]])\n",
    "true_values = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "error = MSE\n",
    "print(error.loss(predictions, true_values))\n",
    "print(error.loss_derivative(predictions, true_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1]]), array([[0., 0.]]), array([[1, 1]]))"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    self.W = np.zeros((input_size, output_size))\n",
    "    self.b = np.zeros(output_size)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    self.X = input\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "  def backward(self, dL_dY):\n",
    "    dL_dX = np.dot(dL_dY, self.W.T)\n",
    "\n",
    "    dL_dW_transpose = np.dot(self.X.T, dL_dY) \n",
    "    dL_dB = dL_dY\n",
    "    print('Weights:\\n', dL_dW_transpose)\n",
    "    print((self.X.T, dL_dY, dL_dX))\n",
    "    print('Biases:\\n', dL_dB)\n",
    "\n",
    "    return dL_dX\n",
    "\n",
    "nn = NeuralNetwork()\n",
    "layer1 = Layer(5, 3)\n",
    "layer2 = Layer(3, 2)\n",
    "nn.add(layer1)\n",
    "nn.add(layer2)\n",
    "\n",
    "input = np.array([[1, 1, 1, 1, 1]])\n",
    "prediction = nn.forward(input)\n",
    "true_value = np.array([[1, 1]])\n",
    "\n",
    "input, prediction, true_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, array([[-1., -1.]]))"
      ]
     },
     "execution_count": 995,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MSE.loss(prediction, true_value)\n",
    "dL_dYhat = MSE.loss_derivative(prediction, true_value)\n",
    "\n",
    "loss, dL_dYhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "(array([[0.],\n",
      "       [0.],\n",
      "       [0.]]), array([[-1., -1.]]), array([[0., 0., 0.]]))\n",
      "Biases:\n",
      " [[-1. -1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 996,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dL_dZ = layer2.backward(dL_dYhat)\n",
    "dL_dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      " [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "(array([[1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1],\n",
      "       [1]]), array([[0., 0., 0.]]), array([[0., 0., 0., 0., 0.]]))\n",
      "Biases:\n",
      " [[0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 997,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.backward(dL_dZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]] [1 1] [[-1. -1.]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (5,) and (1,3) not aligned: 5 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[998], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m true_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[0;32m     52\u001b[0m \u001b[39mprint\u001b[39m(predictions, true_values, MSE\u001b[39m.\u001b[39mloss_derivative(predictions, true_values))\n\u001b[1;32m---> 53\u001b[0m nn\u001b[39m.\u001b[39;49mbackward(MSE\u001b[39m.\u001b[39;49mloss_derivative(predictions, true_values))\n",
      "Cell \u001b[1;32mIn[998], line 42\u001b[0m, in \u001b[0;36mNeuralNetwork.backward\u001b[1;34m(self, dL_dY)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\u001b[39mself\u001b[39m, dL_dY):\n\u001b[0;32m     40\u001b[0m   \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m     41\u001b[0m     \u001b[39m# output derivative of current layer becomes input derivative of next layer\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     dL_dY \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mbackward(dL_dY, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_rate)\n\u001b[0;32m     43\u001b[0m   \u001b[39mreturn\u001b[39;00m dL_dY\n",
      "Cell \u001b[1;32mIn[998], line 16\u001b[0m, in \u001b[0;36mLayer.backward\u001b[1;34m(self, dL_dY, learning_rate)\u001b[0m\n\u001b[0;32m     13\u001b[0m dL_dX \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dL_dY, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mT)\n\u001b[0;32m     15\u001b[0m \u001b[39m# weights and biases derivatives\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m dL_dW_transpose \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX\u001b[39m.\u001b[39;49mT, dL_dY) \n\u001b[0;32m     17\u001b[0m dL_dB \u001b[39m=\u001b[39m dL_dY\n\u001b[0;32m     19\u001b[0m \u001b[39m# update weights and biases\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,) and (1,3) not aligned: 5 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    # initialize weights and biases to 0\n",
    "    self.W = np.zeros((input_size, output_size))\n",
    "    self.b = np.zeros((1, output_size))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    self.X = input # store input to use it during backprogation\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    # input derivative to be used by the previous layer\n",
    "    dL_dX = np.dot(dL_dY, self.W.T)\n",
    "\n",
    "    # weights and biases derivatives\n",
    "    dL_dW_transpose = np.dot(self.X.T, dL_dY) \n",
    "    dL_dB = dL_dY\n",
    "\n",
    "    # update weights and biases\n",
    "    self.W -= learning_rate *  dL_dW_transpose\n",
    "    self.b -= learning_rate * dL_dB\n",
    "\n",
    "    return dL_dX\n",
    "\n",
    "class NeuralNetwork():\n",
    "  def __init__(self, learning_rate=0.01):\n",
    "    self.layers = []\n",
    "    self.learning_rate = learning_rate\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def forward(self, input):\n",
    "    for layer in self.layers:\n",
    "      # output of current layer becomes input of next layer\n",
    "      input = layer.forward(input)\n",
    "    return input\n",
    "  \n",
    "  def backward(self, dL_dY):\n",
    "    for layer in reversed(self.layers):\n",
    "      # output derivative of current layer becomes input derivative of next layer\n",
    "      dL_dY = layer.backward(dL_dY, self.learning_rate)\n",
    "    return dL_dY \n",
    "\n",
    "nn = NeuralNetwork()\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 2))\n",
    "\n",
    "input = np.array([1, 1, 1, 1, 1])\n",
    "predictions = nn.forward(input)\n",
    "true_values = np.array([1, 1])\n",
    "print(predictions, true_values, MSE.loss_derivative(predictions, true_values))\n",
    "nn.backward(MSE.loss_derivative(predictions, true_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69844916 0.69844916 0.69844916]\n",
      " [0.69844916 0.69844916 0.69844916]\n",
      " [0.69844916 0.69844916 0.69844916]\n",
      " [0.69844916 0.69844916 0.69844916]\n",
      " [0.69844916 0.69844916 0.69844916]] [[-0.30155084 -0.30155084 -0.30155084]]\n",
      "[[0.15949004 0.15949004]\n",
      " [0.15949004 0.15949004]\n",
      " [0.15949004 0.15949004]] [[-0.18297055 -0.18297055]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1]]), array([[1.34368168, 1.34368168]]), array([[1, 1]]))"
      ]
     },
     "execution_count": 999,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    # initialize weights and biases\n",
    "    self.W = np.ones((input_size, output_size))\n",
    "    self.b = np.zeros((1, output_size))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    self.X = input # store input to use it during backprogation\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    # input derivative to be used by the previous layer\n",
    "    dL_dX = np.dot(dL_dY, self.W.T)\n",
    "\n",
    "    # weights and biases derivatives\n",
    "    dL_dW_transpose = np.dot(self.X.T, dL_dY) \n",
    "    dL_dB = dL_dY\n",
    "\n",
    "    # update weights and biases\n",
    "    self.W -= learning_rate *  dL_dW_transpose\n",
    "    self.b -= learning_rate * dL_dB\n",
    "\n",
    "    return dL_dX\n",
    "\n",
    "class NeuralNetwork():\n",
    "  def __init__(self, error):\n",
    "    self.layers = []\n",
    "    self.error = error\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def forward(self, input):\n",
    "    for layer in self.layers:\n",
    "      # output of current layer becomes input of next layer\n",
    "      input = layer.forward(input)\n",
    "    return input\n",
    "  \n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    for layer in reversed(self.layers):\n",
    "      # input derivative of current layer becomes output derivative of previous layer\n",
    "      dL_dY = layer.backward(dL_dY, learning_rate)\n",
    "    return dL_dY \n",
    "\n",
    "  def train(self, X, Y, iterations=100, learning_rate=0.01):\n",
    "    for i in range(iterations):\n",
    "      Y_hat = self.forward(X)\n",
    "      dL_dY_hat = self.error.loss_derivative(Y_hat, Y)\n",
    "      self.backward(dL_dY_hat, learning_rate)\n",
    "      \n",
    "      \n",
    "nn = NeuralNetwork(error=MSE)\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 2))\n",
    "\n",
    "X = np.array([[1, 1, 1, 1, 1]])\n",
    "Y = np.array([[1, 1]])\n",
    "nn.train(X, Y, iterations=5)\n",
    "\n",
    "for layer in nn.layers:\n",
    "  print(layer.W, layer.b)\n",
    "\n",
    "X, nn.forward(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.13059845 -0.13059845 -0.13059845]\n",
      " [-0.13059845 -0.13059845 -0.13059845]\n",
      " [-0.13059845 -0.13059845 -0.13059845]\n",
      " [-0.13059845 -0.13059845 -0.13059845]\n",
      " [-0.13059845 -0.13059845 -0.13059845]] [[-0.5729722 -0.5729722 -0.5729722]]\n",
      "[[-0.37990553 -0.63897373]\n",
      " [-0.37990553 -0.63897373]\n",
      " [-0.37990553 -0.63897373]] [[-0.2271764  0.1221751]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2]]),\n",
       " array([[1.1700756 , 2.47225234],\n",
       "        [1.91430169, 3.72398702]]),\n",
       " array([[1, 2],\n",
       "        [2, 4]]))"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    # initialize weights and biases\n",
    "    self.W = np.ones((input_size, output_size))\n",
    "    self.b = np.zeros((1, output_size))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    self.X = input # store input to use it during backprogation\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    # input derivative to be used by the previous layer\n",
    "    dL_dX = np.dot(dL_dY, self.W.T)\n",
    "\n",
    "    # weights and biases derivatives\n",
    "    dL_dW_transpose = np.dot(self.X.T, dL_dY) \n",
    "    dL_dB = dL_dY\n",
    "\n",
    "    # update weights and biases\n",
    "    self.W -= learning_rate *  dL_dW_transpose\n",
    "    self.b -= learning_rate * np.sum(dL_dB, axis=0, keepdims=True)\n",
    "\n",
    "    return dL_dX\n",
    "\n",
    "class NeuralNetwork():\n",
    "  def __init__(self, error):\n",
    "    self.layers = []\n",
    "    self.error = error\n",
    "\n",
    "  def add(self, layer):\n",
    "    self.layers.append(layer)\n",
    "\n",
    "  def forward(self, input):\n",
    "    for layer in self.layers:\n",
    "      # output of current layer becomes input of next layer\n",
    "      input = layer.forward(input)\n",
    "    return input\n",
    "  \n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    for layer in reversed(self.layers):\n",
    "      # input derivative of current layer becomes output derivative of previous layer\n",
    "      dL_dY = layer.backward(dL_dY, learning_rate)\n",
    "    return dL_dY \n",
    "\n",
    "  def train(self, X, Y, iterations=100, learning_rate=0.01):\n",
    "    for i in range(iterations):\n",
    "      Y_hat = self.forward(X)\n",
    "      dL_dY_hat = self.error.loss_derivative(Y_hat, Y)\n",
    "      self.backward(dL_dY_hat, learning_rate)\n",
    "      \n",
    "      \n",
    "nn = NeuralNetwork(error=MSE)\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 2))\n",
    "\n",
    "X = np.array([[1, 1, 1, 1, 1], [2, 2, 2, 2, 2]])\n",
    "Y = np.array([[1, 2], [2, 4]])\n",
    "nn.train(X, Y, iterations=100)\n",
    "\n",
    "for layer in nn.layers:\n",
    "  print(layer.W, layer.b)\n",
    "\n",
    "X, nn.forward(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7046685 0.7046685 0.7046685]\n",
      " [0.7046685 0.7046685 0.7046685]\n",
      " [0.7046685 0.7046685 0.7046685]\n",
      " [0.7046685 0.7046685 0.7046685]\n",
      " [0.7046685 0.7046685 0.7046685]] [[-0.07866017 -0.07866017 -0.07866017]]\n",
      "[[0.09930589 0.19351691]\n",
      " [0.09930589 0.19351691]\n",
      " [0.09930589 0.19351691]] [[-0.05208275 -0.04607867]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2],\n",
       "        [5, 5, 5, 5, 5]]),\n",
       " array([[ 0.97414901,  1.95373418],\n",
       "        [ 2.02381502,  3.99921325],\n",
       "        [ 5.17281304, 10.13565048]]),\n",
       " array([[ 1,  2],\n",
       "        [ 2,  4],\n",
       "        [ 5, 10]]))"
      ]
     },
     "execution_count": 1001,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork(error=MSE)\n",
    "nn.add(Layer(5, 3))\n",
    "nn.add(Layer(3, 2))\n",
    "\n",
    "X = np.array([[1, 1, 1, 1, 1], [2, 2, 2, 2, 2], [5, 5, 5, 5, 5]])\n",
    "Y = np.array([[1, 2], [2, 4], [5, 10]])\n",
    "nn.train(X, Y, iterations=10, learning_rate=0.001)\n",
    "\n",
    "for layer in nn.layers:\n",
    "  print(layer.W, layer.b)\n",
    "\n",
    "X, nn.forward(X), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.73609391 0.73609391]\n",
      " [0.73609391 0.73609391]] [[-0.10073698 -0.10073698]]\n",
      "[[0.31283932]\n",
      " [0.31283932]] [[-0.02498902]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 1]]),\n",
       " array([[-0.088018  ],\n",
       "        [ 0.37254023],\n",
       "        [ 0.37254023],\n",
       "        [ 0.83309847]]),\n",
       " array([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 1002,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XOR_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "XOR_output = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(MSE)\n",
    "nn.add(Layer(2, 2))\n",
    "nn.add(Layer(2, 1))\n",
    "\n",
    "nn.train(XOR_input, XOR_output)\n",
    "\n",
    "for layer in nn.layers:\n",
    "  print(layer.W, layer.b)\n",
    "\n",
    "XOR_input, nn.forward(XOR_input), XOR_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1.])"
      ]
     },
     "execution_count": 1003,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ReLU():\n",
    "  def forward(self, input):\n",
    "    self.X = input\n",
    "    return np.maximum(0, input)\n",
    "  \n",
    "  def backward(self, dL_dY, learning_rate=0):\n",
    "    dY_dX = 1 * (self.X > 0)\n",
    "    # print(('Test', dL_dY))\n",
    "    # print(( dY_dX, self.X))\n",
    "    return dL_dY * dY_dX\n",
    "\n",
    "input = np.array([-1, 0, 0.2, 1])\n",
    "activation = ReLU()\n",
    "activation.forward(input)\n",
    "activation.backward(np.ones(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "  def __init__(self, input_size, output_size):\n",
    "    # initialize weights and biases\n",
    "    self.W = np.random.rand(input_size, output_size) - .5\n",
    "    # self.W = np.ones((input_size, output_size))\n",
    "    # self.b = (np.random.rand(1, output_size) - .5)*100\n",
    "    self.b = np.zeros((1, output_size))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    self.X = input # store input to use it during backprogation\n",
    "    return np.dot(input, self.W) + self.b\n",
    "\n",
    "  def backward(self, dL_dY, learning_rate):\n",
    "    # input derivative to be used by the previous layer\n",
    "    dL_dX = np.dot(dL_dY, self.W.T)\n",
    "\n",
    "    # weights and biases derivatives\n",
    "    dL_dW_transpose = np.dot(self.X.T, dL_dY) \n",
    "    dL_dB = dL_dY\n",
    "\n",
    "    # update weights and biases\n",
    "    self.W -= learning_rate *  dL_dW_transpose\n",
    "    self.b -= learning_rate * np.sum(dL_dB, axis=0, keepdims=True)\n",
    "\n",
    "    return dL_dX\n",
    "\n",
    "XOR_input = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "XOR_output = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "nn = NeuralNetwork(MSE)\n",
    "nn.add(Layer(2, 2))\n",
    "nn.add(ReLU())\n",
    "nn.add(Layer(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48172703]\n",
      " [0.57250466]\n",
      " [0.6112832 ]\n",
      " [0.32930414]]\n",
      "(array([[0.27462132, 0.47141235],\n",
      "       [0.19242211, 0.47139885]]), array([[ 0.16181266, -0.47152328]]))\n",
      "(array([[ 0.47176298],\n",
      "       [-0.79093198]]), array([[0.40538981]]))\n"
     ]
    }
   ],
   "source": [
    "nn.train(XOR_input, XOR_output, iterations=6000 ,learning_rate=0.001)\n",
    "\n",
    "print(nn.forward(XOR_input))\n",
    "\n",
    "for layer in nn.layers:\n",
    "  try:\n",
    "    print((layer.W, layer.b))\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1635,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00]\n",
      " [9.99738648e-01]\n",
      " [9.99749622e-01]\n",
      " [5.25948842e-04]]\n",
      "[[5.94526151e-15]\n",
      " [1.00000000e+00]\n",
      " [1.00000000e+00]\n",
      " [6.10090960e-15]]\n",
      "4.1896998398519884e-29\n"
     ]
    }
   ],
   "source": [
    "layer1 = Layer(2, 2)\n",
    "layer1.W = np.array([[-0.81781753,  0.71323677], [ 0.48803631, -0.71286155]])\n",
    "layer2 = Layer(2, 1)\n",
    "layer2.W = np.array([[2.04849235], [1.40170791]])\n",
    "\n",
    "nn = NeuralNetwork(MSE)\n",
    "nn.add(layer1)\n",
    "nn.add(ReLU())\n",
    "nn.add(layer2)\n",
    "\n",
    "print(nn.forward(XOR_input))\n",
    "\n",
    "# make sure code works as expected\n",
    "nn.train(XOR_input, XOR_output, iterations=10000)\n",
    "print(nn.forward(XOR_input)) \n",
    "print(nn.error.loss(nn.forward(XOR_input), XOR_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1654,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = 0\n",
    "while True:\n",
    "  attempts += 1\n",
    "  nn = NeuralNetwork(MSE)\n",
    "  nn.add(Layer(2,2))\n",
    "  nn.add(ReLU())\n",
    "  nn.add(Layer(2, 1))\n",
    "\n",
    "  nn.train(XOR_input, XOR_output, iterations=1000)\n",
    "  loss = nn.error.loss(nn.forward(XOR_input), XOR_output)\n",
    "  if loss < 0.1:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[-0.73481199,  0.74329449],\n",
      "       [ 0.73471697, -0.74351637]]), array([[0.00022715, 0.00017662]]))\n",
      "(array([[1.07380845],\n",
      "       [1.05957544]]), array([[0.11986323]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.12029428],\n",
       "        [0.90905243],\n",
       "        [0.90762695],\n",
       "        [0.1200051 ]]),\n",
       " 0.01141904476365414,\n",
       " 29)"
      ]
     },
     "execution_count": 1655,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in nn.layers:\n",
    "  try:\n",
    "    print((layer.W, layer.b))\n",
    "  except:\n",
    "    pass\n",
    "  \n",
    "nn.forward(XOR_input), loss, attempts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bedefe99a532ed1dcc567f7f5beb39e3c5be466ed88551f7c83012ec64f38bef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
